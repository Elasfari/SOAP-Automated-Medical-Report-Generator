{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":236505001,"sourceType":"kernelVersion"}],"dockerImageVersionId":31013,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-24T18:43:09.632727Z","iopub.execute_input":"2025-05-24T18:43:09.633228Z","iopub.status.idle":"2025-05-24T18:43:11.547786Z","shell.execute_reply.started":"2025-05-24T18:43:09.633202Z","shell.execute_reply":"2025-05-24T18:43:11.546978Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/nlp-applications/__results__.html\n/kaggle/input/nlp-applications/__notebook__.ipynb\n/kaggle/input/nlp-applications/first_100_test_samples.csv\n/kaggle/input/nlp-applications/__output__.json\n/kaggle/input/nlp-applications/custom.css\n/kaggle/input/nlp-applications/phi2-lora-medical/checkpoint-200/adapter_model.safetensors\n/kaggle/input/nlp-applications/phi2-lora-medical/checkpoint-200/merges.txt\n/kaggle/input/nlp-applications/phi2-lora-medical/checkpoint-200/trainer_state.json\n/kaggle/input/nlp-applications/phi2-lora-medical/checkpoint-200/training_args.bin\n/kaggle/input/nlp-applications/phi2-lora-medical/checkpoint-200/adapter_config.json\n/kaggle/input/nlp-applications/phi2-lora-medical/checkpoint-200/README.md\n/kaggle/input/nlp-applications/phi2-lora-medical/checkpoint-200/tokenizer.json\n/kaggle/input/nlp-applications/phi2-lora-medical/checkpoint-200/vocab.json\n/kaggle/input/nlp-applications/phi2-lora-medical/checkpoint-200/tokenizer_config.json\n/kaggle/input/nlp-applications/phi2-lora-medical/checkpoint-200/scheduler.pt\n/kaggle/input/nlp-applications/phi2-lora-medical/checkpoint-200/special_tokens_map.json\n/kaggle/input/nlp-applications/phi2-lora-medical/checkpoint-200/optimizer.pt\n/kaggle/input/nlp-applications/phi2-lora-medical/checkpoint-200/rng_state.pth\n/kaggle/input/nlp-applications/phi2-lora-medical/checkpoint-200/added_tokens.json\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\n# Load model and tokenizer\nmodel_path = \"/kaggle/input/nlp-applications/phi2-lora-medical/checkpoint-200\"\nmodel = AutoModelForCausalLM.from_pretrained(model_path).to(\"cuda\")\ntokenizer = AutoTokenizer.from_pretrained(model_path)\n\n# Input dialogue\ndialogue = \"\"\"\nThe patient is a 45-year-old male who came to the emergency department with complaints of chest pain and shortness of breath. There is no significant medical history.\nUpon examination, the patient is in moderate distress, with bilateral diffuse rales on his chest. His vital signs are BP 110/70 mmHg, HR 95 bpm, and temperature 98.6Â°F. His oxygen saturation is 95% on room air, and his initial ECG shows nonspecific ST-T changes. A chest x-ray is performed, which reveals a right subpleural infiltrate.\n\"\"\"\n\n# Very clear prompt: NO extra words\nprompt = f\"\"\"You are a medical assistant.\n\nGiven the following dialogue:\n\n{dialogue}\n\nWrite ONLY the SOAP note, starting immediately with:\nS: \nO: \nA: \nP: \nNo explanations, no answers, no comments, no headings.\n\"\"\"\n\n# Tokenize\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n\n# Generate\nwith torch.no_grad():\n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=400,\n        temperature=0.3,\n        top_p=0.95,\n        do_sample=False,\n        eos_token_id=tokenizer.eos_token_id,\n        pad_token_id=tokenizer.pad_token_id,\n    )\n\n# Decode\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# -------------------\n# Postprocessing\n# -------------------\n\n# Find all \"S:\" in text\ns_indices = [i for i in range(len(generated_text)) if generated_text.startswith(\"S:\", i)]\n\nif len(s_indices) >= 2:\n    # Pick the second occurrence (real start)\n    real_start = s_indices[1]\n    generated_text = generated_text[real_start:]\nelif len(s_indices) == 1:\n    real_start = s_indices[0]\n    generated_text = generated_text[real_start:]\nelse:\n    # fallback if no S: found\n    generated_text = generated_text\n\n# Clean extra spaces\ngenerated_soap = generated_text.strip()\n\n# Print final clean SOAP note\nprint(generated_soap)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T18:43:11.549076Z","iopub.execute_input":"2025-05-24T18:43:11.549376Z","iopub.status.idle":"2025-05-24T18:44:23.212738Z","shell.execute_reply.started":"2025-05-24T18:43:11.549359Z","shell.execute_reply":"2025-05-24T18:44:23.211813Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/735 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14c1ac633bfa4a0a861db6bb84bd7f43"}},"metadata":{}},{"name":"stderr","text":"2025-05-24 18:43:26.081662: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1748112206.325194      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1748112206.389336      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/35.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12944e56dd454f17ae8dc65ee311fb8d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16f50d031d1a4a5db9d5504e0d43a045"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/564M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1d7df8e80234d6f88189ae96b1529db"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2ef6ef350564ab280688047f6b64c7e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af6d0c66568e4e85acbc91c9c8c806b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3016565f8355428a832f2bac93eedc9c"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/peft/config.py:162: UserWarning: Unexpected keyword arguments ['alpha_pattern', 'bias', 'corda_config', 'eva_config', 'exclude_modules', 'fan_in_fan_out', 'init_lora_weights', 'layer_replication', 'layers_pattern', 'layers_to_transform', 'loftq_config', 'lora_alpha', 'lora_bias', 'lora_dropout', 'megatron_config', 'megatron_core', 'modules_to_save', 'r', 'rank_pattern', 'target_modules', 'trainable_token_indices', 'use_dora', 'use_rslora'] for class PeftConfig, these are ignored. This probably means that you're loading a configuration file that was saved using a higher version of the library and additional parameters have been introduced since. It is highly recommended to upgrade the PEFT version before continuing (e.g. by running `pip install -U peft`).\n  warnings.warn(\nLoading adapter weights from /kaggle/input/nlp-applications/phi2-lora-medical/checkpoint-200 led to missing keys in the model: model.layers.0.mlp.fc1.lora_A.default.weight, model.layers.0.mlp.fc1.lora_B.default.weight, model.layers.0.mlp.fc2.lora_A.default.weight, model.layers.0.mlp.fc2.lora_B.default.weight, model.layers.1.mlp.fc1.lora_A.default.weight, model.layers.1.mlp.fc1.lora_B.default.weight, model.layers.1.mlp.fc2.lora_A.default.weight, model.layers.1.mlp.fc2.lora_B.default.weight, model.layers.2.mlp.fc1.lora_A.default.weight, model.layers.2.mlp.fc1.lora_B.default.weight, model.layers.2.mlp.fc2.lora_A.default.weight, model.layers.2.mlp.fc2.lora_B.default.weight, model.layers.3.mlp.fc1.lora_A.default.weight, model.layers.3.mlp.fc1.lora_B.default.weight, model.layers.3.mlp.fc2.lora_A.default.weight, model.layers.3.mlp.fc2.lora_B.default.weight, model.layers.4.mlp.fc1.lora_A.default.weight, model.layers.4.mlp.fc1.lora_B.default.weight, model.layers.4.mlp.fc2.lora_A.default.weight, model.layers.4.mlp.fc2.lora_B.default.weight, model.layers.5.mlp.fc1.lora_A.default.weight, model.layers.5.mlp.fc1.lora_B.default.weight, model.layers.5.mlp.fc2.lora_A.default.weight, model.layers.5.mlp.fc2.lora_B.default.weight, model.layers.6.mlp.fc1.lora_A.default.weight, model.layers.6.mlp.fc1.lora_B.default.weight, model.layers.6.mlp.fc2.lora_A.default.weight, model.layers.6.mlp.fc2.lora_B.default.weight, model.layers.7.mlp.fc1.lora_A.default.weight, model.layers.7.mlp.fc1.lora_B.default.weight, model.layers.7.mlp.fc2.lora_A.default.weight, model.layers.7.mlp.fc2.lora_B.default.weight, model.layers.8.mlp.fc1.lora_A.default.weight, model.layers.8.mlp.fc1.lora_B.default.weight, model.layers.8.mlp.fc2.lora_A.default.weight, model.layers.8.mlp.fc2.lora_B.default.weight, model.layers.9.mlp.fc1.lora_A.default.weight, model.layers.9.mlp.fc1.lora_B.default.weight, model.layers.9.mlp.fc2.lora_A.default.weight, model.layers.9.mlp.fc2.lora_B.default.weight, model.layers.10.mlp.fc1.lora_A.default.weight, model.layers.10.mlp.fc1.lora_B.default.weight, model.layers.10.mlp.fc2.lora_A.default.weight, model.layers.10.mlp.fc2.lora_B.default.weight, model.layers.11.mlp.fc1.lora_A.default.weight, model.layers.11.mlp.fc1.lora_B.default.weight, model.layers.11.mlp.fc2.lora_A.default.weight, model.layers.11.mlp.fc2.lora_B.default.weight, model.layers.12.mlp.fc1.lora_A.default.weight, model.layers.12.mlp.fc1.lora_B.default.weight, model.layers.12.mlp.fc2.lora_A.default.weight, model.layers.12.mlp.fc2.lora_B.default.weight, model.layers.13.mlp.fc1.lora_A.default.weight, model.layers.13.mlp.fc1.lora_B.default.weight, model.layers.13.mlp.fc2.lora_A.default.weight, model.layers.13.mlp.fc2.lora_B.default.weight, model.layers.14.mlp.fc1.lora_A.default.weight, model.layers.14.mlp.fc1.lora_B.default.weight, model.layers.14.mlp.fc2.lora_A.default.weight, model.layers.14.mlp.fc2.lora_B.default.weight, model.layers.15.mlp.fc1.lora_A.default.weight, model.layers.15.mlp.fc1.lora_B.default.weight, model.layers.15.mlp.fc2.lora_A.default.weight, model.layers.15.mlp.fc2.lora_B.default.weight, model.layers.16.mlp.fc1.lora_A.default.weight, model.layers.16.mlp.fc1.lora_B.default.weight, model.layers.16.mlp.fc2.lora_A.default.weight, model.layers.16.mlp.fc2.lora_B.default.weight, model.layers.17.mlp.fc1.lora_A.default.weight, model.layers.17.mlp.fc1.lora_B.default.weight, model.layers.17.mlp.fc2.lora_A.default.weight, model.layers.17.mlp.fc2.lora_B.default.weight, model.layers.18.mlp.fc1.lora_A.default.weight, model.layers.18.mlp.fc1.lora_B.default.weight, model.layers.18.mlp.fc2.lora_A.default.weight, model.layers.18.mlp.fc2.lora_B.default.weight, model.layers.19.mlp.fc1.lora_A.default.weight, model.layers.19.mlp.fc1.lora_B.default.weight, model.layers.19.mlp.fc2.lora_A.default.weight, model.layers.19.mlp.fc2.lora_B.default.weight, model.layers.20.mlp.fc1.lora_A.default.weight, model.layers.20.mlp.fc1.lora_B.default.weight, model.layers.20.mlp.fc2.lora_A.default.weight, model.layers.20.mlp.fc2.lora_B.default.weight, model.layers.21.mlp.fc1.lora_A.default.weight, model.layers.21.mlp.fc1.lora_B.default.weight, model.layers.21.mlp.fc2.lora_A.default.weight, model.layers.21.mlp.fc2.lora_B.default.weight, model.layers.22.mlp.fc1.lora_A.default.weight, model.layers.22.mlp.fc1.lora_B.default.weight, model.layers.22.mlp.fc2.lora_A.default.weight, model.layers.22.mlp.fc2.lora_B.default.weight, model.layers.23.mlp.fc1.lora_A.default.weight, model.layers.23.mlp.fc1.lora_B.default.weight, model.layers.23.mlp.fc2.lora_A.default.weight, model.layers.23.mlp.fc2.lora_B.default.weight, model.layers.24.mlp.fc1.lora_A.default.weight, model.layers.24.mlp.fc1.lora_B.default.weight, model.layers.24.mlp.fc2.lora_A.default.weight, model.layers.24.mlp.fc2.lora_B.default.weight, model.layers.25.mlp.fc1.lora_A.default.weight, model.layers.25.mlp.fc1.lora_B.default.weight, model.layers.25.mlp.fc2.lora_A.default.weight, model.layers.25.mlp.fc2.lora_B.default.weight, model.layers.26.mlp.fc1.lora_A.default.weight, model.layers.26.mlp.fc1.lora_B.default.weight, model.layers.26.mlp.fc2.lora_A.default.weight, model.layers.26.mlp.fc2.lora_B.default.weight, model.layers.27.mlp.fc1.lora_A.default.weight, model.layers.27.mlp.fc1.lora_B.default.weight, model.layers.27.mlp.fc2.lora_A.default.weight, model.layers.27.mlp.fc2.lora_B.default.weight, model.layers.28.mlp.fc1.lora_A.default.weight, model.layers.28.mlp.fc1.lora_B.default.weight, model.layers.28.mlp.fc2.lora_A.default.weight, model.layers.28.mlp.fc2.lora_B.default.weight, model.layers.29.mlp.fc1.lora_A.default.weight, model.layers.29.mlp.fc1.lora_B.default.weight, model.layers.29.mlp.fc2.lora_A.default.weight, model.layers.29.mlp.fc2.lora_B.default.weight, model.layers.30.mlp.fc1.lora_A.default.weight, model.layers.30.mlp.fc1.lora_B.default.weight, model.layers.30.mlp.fc2.lora_A.default.weight, model.layers.30.mlp.fc2.lora_B.default.weight, model.layers.31.mlp.fc1.lora_A.default.weight, model.layers.31.mlp.fc1.lora_B.default.weight, model.layers.31.mlp.fc2.lora_A.default.weight, model.layers.31.mlp.fc2.lora_B.default.weight\n/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.3` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"S: The patient is a 45-year-old male who presented to the emergency department with complaints of chest pain and shortness of breath. He has no significant medical history.\nO: Upon examination, the patient is in moderate distress, with bilateral diffuse rales on his chest. His vital signs are BP 110/70 mmHg, HR 95 bpm, and temperature 98.6Â°F. His oxygen saturation is 95% on room air, and his initial ECG shows nonspecific ST-T changes. A chest x-ray reveals a right subpleural infiltrate.\nA: The patient's chest pain and shortness of breath are concerning for a possible pulmonary embolism. The bilateral diffuse rales on his chest and the right subpleural infiltrate on the chest x-ray further support this suspicion. The patient's vital signs are within normal limits, and his oxygen saturation is adequate. The nonspecific ST-T changes on the ECG are not specific for a pulmonary embolism but could be indicative of other cardiac conditions.\nP: The patient's symptoms and physical examination findings suggest a possible pulmonary embolism. Further diagnostic tests, such as a computed tomography pulmonary angiography (CTPA) or a ventilation-perfusion scan, should be performed to confirm the diagnosis. In the meantime, the patient should be started on anticoagulant therapy, such as heparin, to prevent further clot formation. Close monitoring of his vital signs, oxygen saturation, and ECG is necessary.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\n# 1. Load model and tokenizer\nmodel_path = \"/kaggle/input/nlp-applications/phi2-lora-medical/checkpoint-200\"\nmodel = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, device_map=\"auto\")\ntokenizer = AutoTokenizer.from_pretrained(model_path)\n\n# 2. Load test data\ntest_data = pd.read_csv(\"/kaggle/input/nlp-applications/first_100_test_samples.csv\")\n\n# 3. Take only the first dialogue\nfirst_dialogue = test_data.iloc[0]['dialogue']\n\n# 4. Prepare the prompt\nprompt = f\"\"\"You are a medical assistant.\n\nGiven the following dialogue:\n\n{dialogue}\n\nWrite ONLY the SOAP note, starting immediately with:\nS: \nO: \nA: \nP: \nNo explanations, no answers, no comments, no headings.\n\"\"\"\n\n# 5. Tokenize the prompt\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n\n# 6. Generate the SOAP report\nwith torch.no_grad():\n    output = model.generate(**inputs, max_new_tokens=500)\ngenerated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n\n# 7. Extract only the SOAP report\nstart_index = generated_text.find(\"S:\")\nfinal_soap_report = generated_text[start_index:].strip() if start_index != -1 else generated_text.strip()\n\n# 8. Save into CSV with exactly two columns\ndf_save = pd.DataFrame({\n    'dialogue': [first_dialogue],\n    'generated_soap_report': [final_soap_report]\n})\n\ndf_save.to_csv(\"first_generated_soap_report.csv\", index=False)\n\nprint(\"done\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T18:44:23.213687Z","iopub.execute_input":"2025-05-24T18:44:23.215043Z","iopub.status.idle":"2025-05-24T18:44:44.879835Z","shell.execute_reply.started":"2025-05-24T18:44:23.215011Z","shell.execute_reply":"2025-05-24T18:44:44.879053Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34d826fe8ab943c3accf4acbbc1c07ec"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/peft/config.py:162: UserWarning: Unexpected keyword arguments ['alpha_pattern', 'bias', 'corda_config', 'eva_config', 'exclude_modules', 'fan_in_fan_out', 'init_lora_weights', 'layer_replication', 'layers_pattern', 'layers_to_transform', 'loftq_config', 'lora_alpha', 'lora_bias', 'lora_dropout', 'megatron_config', 'megatron_core', 'modules_to_save', 'r', 'rank_pattern', 'target_modules', 'trainable_token_indices', 'use_dora', 'use_rslora'] for class PeftConfig, these are ignored. This probably means that you're loading a configuration file that was saved using a higher version of the library and additional parameters have been introduced since. It is highly recommended to upgrade the PEFT version before continuing (e.g. by running `pip install -U peft`).\n  warnings.warn(\nLoading adapter weights from /kaggle/input/nlp-applications/phi2-lora-medical/checkpoint-200 led to missing keys in the model: model.layers.0.mlp.fc1.lora_A.default.weight, model.layers.0.mlp.fc1.lora_B.default.weight, model.layers.0.mlp.fc2.lora_A.default.weight, model.layers.0.mlp.fc2.lora_B.default.weight, model.layers.1.mlp.fc1.lora_A.default.weight, model.layers.1.mlp.fc1.lora_B.default.weight, model.layers.1.mlp.fc2.lora_A.default.weight, model.layers.1.mlp.fc2.lora_B.default.weight, model.layers.2.mlp.fc1.lora_A.default.weight, model.layers.2.mlp.fc1.lora_B.default.weight, model.layers.2.mlp.fc2.lora_A.default.weight, model.layers.2.mlp.fc2.lora_B.default.weight, model.layers.3.mlp.fc1.lora_A.default.weight, model.layers.3.mlp.fc1.lora_B.default.weight, model.layers.3.mlp.fc2.lora_A.default.weight, model.layers.3.mlp.fc2.lora_B.default.weight, model.layers.4.mlp.fc1.lora_A.default.weight, model.layers.4.mlp.fc1.lora_B.default.weight, model.layers.4.mlp.fc2.lora_A.default.weight, model.layers.4.mlp.fc2.lora_B.default.weight, model.layers.5.mlp.fc1.lora_A.default.weight, model.layers.5.mlp.fc1.lora_B.default.weight, model.layers.5.mlp.fc2.lora_A.default.weight, model.layers.5.mlp.fc2.lora_B.default.weight, model.layers.6.mlp.fc1.lora_A.default.weight, model.layers.6.mlp.fc1.lora_B.default.weight, model.layers.6.mlp.fc2.lora_A.default.weight, model.layers.6.mlp.fc2.lora_B.default.weight, model.layers.7.mlp.fc1.lora_A.default.weight, model.layers.7.mlp.fc1.lora_B.default.weight, model.layers.7.mlp.fc2.lora_A.default.weight, model.layers.7.mlp.fc2.lora_B.default.weight, model.layers.8.mlp.fc1.lora_A.default.weight, model.layers.8.mlp.fc1.lora_B.default.weight, model.layers.8.mlp.fc2.lora_A.default.weight, model.layers.8.mlp.fc2.lora_B.default.weight, model.layers.9.mlp.fc1.lora_A.default.weight, model.layers.9.mlp.fc1.lora_B.default.weight, model.layers.9.mlp.fc2.lora_A.default.weight, model.layers.9.mlp.fc2.lora_B.default.weight, model.layers.10.mlp.fc1.lora_A.default.weight, model.layers.10.mlp.fc1.lora_B.default.weight, model.layers.10.mlp.fc2.lora_A.default.weight, model.layers.10.mlp.fc2.lora_B.default.weight, model.layers.11.mlp.fc1.lora_A.default.weight, model.layers.11.mlp.fc1.lora_B.default.weight, model.layers.11.mlp.fc2.lora_A.default.weight, model.layers.11.mlp.fc2.lora_B.default.weight, model.layers.12.mlp.fc1.lora_A.default.weight, model.layers.12.mlp.fc1.lora_B.default.weight, model.layers.12.mlp.fc2.lora_A.default.weight, model.layers.12.mlp.fc2.lora_B.default.weight, model.layers.13.mlp.fc1.lora_A.default.weight, model.layers.13.mlp.fc1.lora_B.default.weight, model.layers.13.mlp.fc2.lora_A.default.weight, model.layers.13.mlp.fc2.lora_B.default.weight, model.layers.14.mlp.fc1.lora_A.default.weight, model.layers.14.mlp.fc1.lora_B.default.weight, model.layers.14.mlp.fc2.lora_A.default.weight, model.layers.14.mlp.fc2.lora_B.default.weight, model.layers.15.mlp.fc1.lora_A.default.weight, model.layers.15.mlp.fc1.lora_B.default.weight, model.layers.15.mlp.fc2.lora_A.default.weight, model.layers.15.mlp.fc2.lora_B.default.weight, model.layers.16.mlp.fc1.lora_A.default.weight, model.layers.16.mlp.fc1.lora_B.default.weight, model.layers.16.mlp.fc2.lora_A.default.weight, model.layers.16.mlp.fc2.lora_B.default.weight, model.layers.17.mlp.fc1.lora_A.default.weight, model.layers.17.mlp.fc1.lora_B.default.weight, model.layers.17.mlp.fc2.lora_A.default.weight, model.layers.17.mlp.fc2.lora_B.default.weight, model.layers.18.mlp.fc1.lora_A.default.weight, model.layers.18.mlp.fc1.lora_B.default.weight, model.layers.18.mlp.fc2.lora_A.default.weight, model.layers.18.mlp.fc2.lora_B.default.weight, model.layers.19.mlp.fc1.lora_A.default.weight, model.layers.19.mlp.fc1.lora_B.default.weight, model.layers.19.mlp.fc2.lora_A.default.weight, model.layers.19.mlp.fc2.lora_B.default.weight, model.layers.20.mlp.fc1.lora_A.default.weight, model.layers.20.mlp.fc1.lora_B.default.weight, model.layers.20.mlp.fc2.lora_A.default.weight, model.layers.20.mlp.fc2.lora_B.default.weight, model.layers.21.mlp.fc1.lora_A.default.weight, model.layers.21.mlp.fc1.lora_B.default.weight, model.layers.21.mlp.fc2.lora_A.default.weight, model.layers.21.mlp.fc2.lora_B.default.weight, model.layers.22.mlp.fc1.lora_A.default.weight, model.layers.22.mlp.fc1.lora_B.default.weight, model.layers.22.mlp.fc2.lora_A.default.weight, model.layers.22.mlp.fc2.lora_B.default.weight, model.layers.23.mlp.fc1.lora_A.default.weight, model.layers.23.mlp.fc1.lora_B.default.weight, model.layers.23.mlp.fc2.lora_A.default.weight, model.layers.23.mlp.fc2.lora_B.default.weight, model.layers.24.mlp.fc1.lora_A.default.weight, model.layers.24.mlp.fc1.lora_B.default.weight, model.layers.24.mlp.fc2.lora_A.default.weight, model.layers.24.mlp.fc2.lora_B.default.weight, model.layers.25.mlp.fc1.lora_A.default.weight, model.layers.25.mlp.fc1.lora_B.default.weight, model.layers.25.mlp.fc2.lora_A.default.weight, model.layers.25.mlp.fc2.lora_B.default.weight, model.layers.26.mlp.fc1.lora_A.default.weight, model.layers.26.mlp.fc1.lora_B.default.weight, model.layers.26.mlp.fc2.lora_A.default.weight, model.layers.26.mlp.fc2.lora_B.default.weight, model.layers.27.mlp.fc1.lora_A.default.weight, model.layers.27.mlp.fc1.lora_B.default.weight, model.layers.27.mlp.fc2.lora_A.default.weight, model.layers.27.mlp.fc2.lora_B.default.weight, model.layers.28.mlp.fc1.lora_A.default.weight, model.layers.28.mlp.fc1.lora_B.default.weight, model.layers.28.mlp.fc2.lora_A.default.weight, model.layers.28.mlp.fc2.lora_B.default.weight, model.layers.29.mlp.fc1.lora_A.default.weight, model.layers.29.mlp.fc1.lora_B.default.weight, model.layers.29.mlp.fc2.lora_A.default.weight, model.layers.29.mlp.fc2.lora_B.default.weight, model.layers.30.mlp.fc1.lora_A.default.weight, model.layers.30.mlp.fc1.lora_B.default.weight, model.layers.30.mlp.fc2.lora_A.default.weight, model.layers.30.mlp.fc2.lora_B.default.weight, model.layers.31.mlp.fc1.lora_A.default.weight, model.layers.31.mlp.fc1.lora_B.default.weight, model.layers.31.mlp.fc2.lora_A.default.weight, model.layers.31.mlp.fc2.lora_B.default.weight\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"done\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv(\"/kaggle/working/first_generated_soap_report.csv\")\nprint(df[\"generated_soap_report\"].head(1).max())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T18:44:44.881573Z","iopub.execute_input":"2025-05-24T18:44:44.881822Z","iopub.status.idle":"2025-05-24T18:44:44.888325Z","shell.execute_reply.started":"2025-05-24T18:44:44.881804Z","shell.execute_reply":"2025-05-24T18:44:44.887741Z"}},"outputs":[{"name":"stdout","text":"S: \nO: \nA: \nP: \nNo explanations, no answers, no comments, no headings.\n\nAnswer:\nS: The patient is a 45-year-old male who presented to the emergency department with complaints of chest pain and shortness of breath. He has no significant medical history.\nO: Upon examination, the patient is in moderate distress, with bilateral diffuse rales on his chest. His vital signs are BP 110/70 mmHg, HR 95 bpm, and temperature 98.6Â°F. His oxygen saturation is 95% on room air, and his initial ECG shows nonspecific ST-T changes. A chest x-ray reveals a right subpleural infiltrate.\nA: The patient's chest pain and shortness of breath are concerning for a possible pulmonary embolism. The bilateral diffuse rales on his chest and the right subpleural infiltrate on the chest x-ray further support this suspicion. The patient's vital signs are within normal limits, and his oxygen saturation is adequate. The nonspecific ST-T changes on the ECG are not specific for a pulmonary embolism but could be indicative of other cardiac conditions.\nP: The patient's symptoms and physical examination findings suggest a possible pulmonary embolism. Further diagnostic tests, such as a computed tomography pulmonary angiography (CTPA) or a ventilation-perfusion scan, should be performed to confirm the diagnosis. The patient should be started on anticoagulant therapy, such as heparin, while awaiting the results of these tests.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import pandas as pd\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\n# 1. Load model and tokenizer\nmodel_path = \"/kaggle/input/nlp-applications/phi2-lora-medical/checkpoint-200\"\nmodel = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, device_map=\"auto\")\ntokenizer = AutoTokenizer.from_pretrained(model_path)\n\n# 2. Load test data\ntest_data = pd.read_csv(\"/kaggle/input/nlp-applications/first_100_test_samples.csv\")\n\n# 3. Function to generate clean SOAP notes\ndef generate_soap(dialogue):\n    # Very clear prompt with strict formatting instructions\n    prompt = f\"\"\"You are a medical assistant. Given this doctor-patient dialogue, generate ONLY a SOAP note in this exact format:\n\nS: [Subjective findings - patient's complaints and history]\nO: [Objective findings - exam results, vitals, test results]\nA: [Assessment - diagnosis/differential]\nP: [Plan - treatment recommendations]\n\nDialogue:\n{dialogue}\n\nSOAP Note:\nS: \"\"\"\n    \n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=500,\n            temperature=0.3,  # Lower temperature for more focused output\n            do_sample=True,\n            eos_token_id=tokenizer.eos_token_id,\n            pad_token_id=tokenizer.pad_token_id,\n        )\n    \n    full_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    # Extract just the SOAP note part\n    start_idx = full_text.find(\"S: \")\n    if start_idx == -1:\n        return \"S: \\nO: \\nA: \\nP: \"  # Fallback if format not found\n    \n    soap_note = full_text[start_idx:]\n    \n    # Ensure all sections are present\n    for section in [\"O: \", \"A: \", \"P: \"]:\n        if section not in soap_note:\n            soap_note += f\"\\n{section}\"\n    \n    return soap_note\n\n# 4. Process all samples and save to CSV\nresults = []\nfor idx, row in test_data.iterrows():\n    dialogue = row['dialogue']\n    soap_note = generate_soap(dialogue)\n    results.append({'dialogue': dialogue, 'generated_soap': soap_note})\n    \n# 5. Create DataFrame and save\ndf_results = pd.DataFrame(results)\ndf_results.to_csv(\"/kaggle/working/soap_notes_results.csv\", index=False)\n\nprint(\"SOAP notes generated and saved successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T18:44:44.889003Z","iopub.execute_input":"2025-05-24T18:44:44.889169Z","iopub.status.idle":"2025-05-24T19:32:43.579476Z","shell.execute_reply.started":"2025-05-24T18:44:44.889156Z","shell.execute_reply":"2025-05-24T19:32:43.578808Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3189f1926014e68be88b5b2be3edff5"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/peft/config.py:162: UserWarning: Unexpected keyword arguments ['alpha_pattern', 'bias', 'corda_config', 'eva_config', 'exclude_modules', 'fan_in_fan_out', 'init_lora_weights', 'layer_replication', 'layers_pattern', 'layers_to_transform', 'loftq_config', 'lora_alpha', 'lora_bias', 'lora_dropout', 'megatron_config', 'megatron_core', 'modules_to_save', 'r', 'rank_pattern', 'target_modules', 'trainable_token_indices', 'use_dora', 'use_rslora'] for class PeftConfig, these are ignored. This probably means that you're loading a configuration file that was saved using a higher version of the library and additional parameters have been introduced since. It is highly recommended to upgrade the PEFT version before continuing (e.g. by running `pip install -U peft`).\n  warnings.warn(\nLoading adapter weights from /kaggle/input/nlp-applications/phi2-lora-medical/checkpoint-200 led to missing keys in the model: model.layers.0.mlp.fc1.lora_A.default.weight, model.layers.0.mlp.fc1.lora_B.default.weight, model.layers.0.mlp.fc2.lora_A.default.weight, model.layers.0.mlp.fc2.lora_B.default.weight, model.layers.1.mlp.fc1.lora_A.default.weight, model.layers.1.mlp.fc1.lora_B.default.weight, model.layers.1.mlp.fc2.lora_A.default.weight, model.layers.1.mlp.fc2.lora_B.default.weight, model.layers.2.mlp.fc1.lora_A.default.weight, model.layers.2.mlp.fc1.lora_B.default.weight, model.layers.2.mlp.fc2.lora_A.default.weight, model.layers.2.mlp.fc2.lora_B.default.weight, model.layers.3.mlp.fc1.lora_A.default.weight, model.layers.3.mlp.fc1.lora_B.default.weight, model.layers.3.mlp.fc2.lora_A.default.weight, model.layers.3.mlp.fc2.lora_B.default.weight, model.layers.4.mlp.fc1.lora_A.default.weight, model.layers.4.mlp.fc1.lora_B.default.weight, model.layers.4.mlp.fc2.lora_A.default.weight, model.layers.4.mlp.fc2.lora_B.default.weight, model.layers.5.mlp.fc1.lora_A.default.weight, model.layers.5.mlp.fc1.lora_B.default.weight, model.layers.5.mlp.fc2.lora_A.default.weight, model.layers.5.mlp.fc2.lora_B.default.weight, model.layers.6.mlp.fc1.lora_A.default.weight, model.layers.6.mlp.fc1.lora_B.default.weight, model.layers.6.mlp.fc2.lora_A.default.weight, model.layers.6.mlp.fc2.lora_B.default.weight, model.layers.7.mlp.fc1.lora_A.default.weight, model.layers.7.mlp.fc1.lora_B.default.weight, model.layers.7.mlp.fc2.lora_A.default.weight, model.layers.7.mlp.fc2.lora_B.default.weight, model.layers.8.mlp.fc1.lora_A.default.weight, model.layers.8.mlp.fc1.lora_B.default.weight, model.layers.8.mlp.fc2.lora_A.default.weight, model.layers.8.mlp.fc2.lora_B.default.weight, model.layers.9.mlp.fc1.lora_A.default.weight, model.layers.9.mlp.fc1.lora_B.default.weight, model.layers.9.mlp.fc2.lora_A.default.weight, model.layers.9.mlp.fc2.lora_B.default.weight, model.layers.10.mlp.fc1.lora_A.default.weight, model.layers.10.mlp.fc1.lora_B.default.weight, model.layers.10.mlp.fc2.lora_A.default.weight, model.layers.10.mlp.fc2.lora_B.default.weight, model.layers.11.mlp.fc1.lora_A.default.weight, model.layers.11.mlp.fc1.lora_B.default.weight, model.layers.11.mlp.fc2.lora_A.default.weight, model.layers.11.mlp.fc2.lora_B.default.weight, model.layers.12.mlp.fc1.lora_A.default.weight, model.layers.12.mlp.fc1.lora_B.default.weight, model.layers.12.mlp.fc2.lora_A.default.weight, model.layers.12.mlp.fc2.lora_B.default.weight, model.layers.13.mlp.fc1.lora_A.default.weight, model.layers.13.mlp.fc1.lora_B.default.weight, model.layers.13.mlp.fc2.lora_A.default.weight, model.layers.13.mlp.fc2.lora_B.default.weight, model.layers.14.mlp.fc1.lora_A.default.weight, model.layers.14.mlp.fc1.lora_B.default.weight, model.layers.14.mlp.fc2.lora_A.default.weight, model.layers.14.mlp.fc2.lora_B.default.weight, model.layers.15.mlp.fc1.lora_A.default.weight, model.layers.15.mlp.fc1.lora_B.default.weight, model.layers.15.mlp.fc2.lora_A.default.weight, model.layers.15.mlp.fc2.lora_B.default.weight, model.layers.16.mlp.fc1.lora_A.default.weight, model.layers.16.mlp.fc1.lora_B.default.weight, model.layers.16.mlp.fc2.lora_A.default.weight, model.layers.16.mlp.fc2.lora_B.default.weight, model.layers.17.mlp.fc1.lora_A.default.weight, model.layers.17.mlp.fc1.lora_B.default.weight, model.layers.17.mlp.fc2.lora_A.default.weight, model.layers.17.mlp.fc2.lora_B.default.weight, model.layers.18.mlp.fc1.lora_A.default.weight, model.layers.18.mlp.fc1.lora_B.default.weight, model.layers.18.mlp.fc2.lora_A.default.weight, model.layers.18.mlp.fc2.lora_B.default.weight, model.layers.19.mlp.fc1.lora_A.default.weight, model.layers.19.mlp.fc1.lora_B.default.weight, model.layers.19.mlp.fc2.lora_A.default.weight, model.layers.19.mlp.fc2.lora_B.default.weight, model.layers.20.mlp.fc1.lora_A.default.weight, model.layers.20.mlp.fc1.lora_B.default.weight, model.layers.20.mlp.fc2.lora_A.default.weight, model.layers.20.mlp.fc2.lora_B.default.weight, model.layers.21.mlp.fc1.lora_A.default.weight, model.layers.21.mlp.fc1.lora_B.default.weight, model.layers.21.mlp.fc2.lora_A.default.weight, model.layers.21.mlp.fc2.lora_B.default.weight, model.layers.22.mlp.fc1.lora_A.default.weight, model.layers.22.mlp.fc1.lora_B.default.weight, model.layers.22.mlp.fc2.lora_A.default.weight, model.layers.22.mlp.fc2.lora_B.default.weight, model.layers.23.mlp.fc1.lora_A.default.weight, model.layers.23.mlp.fc1.lora_B.default.weight, model.layers.23.mlp.fc2.lora_A.default.weight, model.layers.23.mlp.fc2.lora_B.default.weight, model.layers.24.mlp.fc1.lora_A.default.weight, model.layers.24.mlp.fc1.lora_B.default.weight, model.layers.24.mlp.fc2.lora_A.default.weight, model.layers.24.mlp.fc2.lora_B.default.weight, model.layers.25.mlp.fc1.lora_A.default.weight, model.layers.25.mlp.fc1.lora_B.default.weight, model.layers.25.mlp.fc2.lora_A.default.weight, model.layers.25.mlp.fc2.lora_B.default.weight, model.layers.26.mlp.fc1.lora_A.default.weight, model.layers.26.mlp.fc1.lora_B.default.weight, model.layers.26.mlp.fc2.lora_A.default.weight, model.layers.26.mlp.fc2.lora_B.default.weight, model.layers.27.mlp.fc1.lora_A.default.weight, model.layers.27.mlp.fc1.lora_B.default.weight, model.layers.27.mlp.fc2.lora_A.default.weight, model.layers.27.mlp.fc2.lora_B.default.weight, model.layers.28.mlp.fc1.lora_A.default.weight, model.layers.28.mlp.fc1.lora_B.default.weight, model.layers.28.mlp.fc2.lora_A.default.weight, model.layers.28.mlp.fc2.lora_B.default.weight, model.layers.29.mlp.fc1.lora_A.default.weight, model.layers.29.mlp.fc1.lora_B.default.weight, model.layers.29.mlp.fc2.lora_A.default.weight, model.layers.29.mlp.fc2.lora_B.default.weight, model.layers.30.mlp.fc1.lora_A.default.weight, model.layers.30.mlp.fc1.lora_B.default.weight, model.layers.30.mlp.fc2.lora_A.default.weight, model.layers.30.mlp.fc2.lora_B.default.weight, model.layers.31.mlp.fc1.lora_A.default.weight, model.layers.31.mlp.fc1.lora_B.default.weight, model.layers.31.mlp.fc2.lora_A.default.weight, model.layers.31.mlp.fc2.lora_B.default.weight\n","output_type":"stream"},{"name":"stdout","text":"SOAP notes generated and saved successfully!\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"pip install rouge-score bert-score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T19:42:19.995058Z","iopub.execute_input":"2025-05-24T19:42:19.995347Z","iopub.status.idle":"2025-05-24T19:43:48.200169Z","shell.execute_reply.started":"2025-05-24T19:42:19.995324Z","shell.execute_reply":"2025-05-24T19:43:48.199305Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting rouge-score\n  Using cached rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting bert-score\n  Using cached bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\nRequirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge-score) (3.9.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.17.0)\nRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.5.1+cu124)\nRequirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.2.3)\nRequirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from bert-score) (4.51.1)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.32.3)\nRequirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.11/dist-packages (from bert-score) (4.67.1)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from bert-score) (3.7.5)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from bert-score) (24.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->rouge-score) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->rouge-score) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->rouge-score) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->rouge-score) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->rouge-score) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->rouge-score) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.0.0->bert-score)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.0.0->bert-score)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.0.0->bert-score)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.0.0->bert-score)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.0.0->bert-score)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.0.0->bert-score)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.0.0->bert-score)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert-score) (1.3.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.30.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.5.2)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (4.56.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (1.4.8)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (11.1.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (3.2.1)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (8.1.8)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (1.4.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (2025.1.31)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rouge-score) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rouge-score) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->rouge-score) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->rouge-score) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->rouge-score) (2024.2.0)\nDownloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: rouge-score\n  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=d0594479df733159fdda68ea9df09fec6564ea87acd4cfc586855c9ab8f4b40a\n  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\nSuccessfully built rouge-score\nInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, rouge-score, bert-score\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.9.90\n    Uninstalling nvidia-curand-cu12-10.3.9.90:\n      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed bert-score-0.3.13 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 rouge-score-0.1.2\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom rouge_score import rouge_scorer\nfrom bert_score import score\n\n# 1. Load your data (first 100 rows)\ndf = pd.read_csv(\"soap_notes_results.csv\").head(100)\n\n# 2. Create temporary reference notes (for demonstration only)\n# WARNING: These are auto-generated so scores won't be meaningful\nprint(\"Creating temporary reference notes for demonstration...\")\ndf[\"reference_soap\"] = df[\"dialogue\"].apply(lambda x: \n    \"S: Patient reports symptoms\\nO: Examination findings\\nA: Assessment\\nP: Plan\")\n\n# 3. Clean data\ndf = df.dropna(subset=[\"generated_soap\", \"reference_soap\"])\n\n# 4. Calculate ROUGE scores\nprint(\"\\nCalculating ROUGE scores...\")\nscorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeLsum'], use_stemmer=True)\n\nrouge_results = []\nfor _, row in df.iterrows():\n    scores = scorer.score(row[\"reference_soap\"], row[\"generated_soap\"])\n    rouge_results.append({\n        'rouge1': scores['rouge1'].fmeasure,\n        'rouge2': scores['rouge2'].fmeasure,\n        'rougeLsum': scores['rougeLsum'].fmeasure\n    })\n\nrouge_df = pd.DataFrame(rouge_results)\ndf = pd.concat([df, rouge_df], axis=1)\n\n# 5. Calculate BERTScore\nprint(\"Calculating BERTScore (this may take a few minutes)...\")\n_, _, bert_f1 = score(df[\"generated_soap\"].tolist(), \n                     df[\"reference_soap\"].tolist(), \n                     lang=\"en\")\ndf[\"bert_f1\"] = bert_f1\n\n# 6. Show results\nprint(\"\\nEvaluation Results (First 100 Reports):\")\nprint(f\"ROUGE-1 Average: {df['rouge1'].mean():.3f}\")\nprint(f\"ROUGE-2 Average: {df['rouge2'].mean():.3f}\")\nprint(f\"ROUGE-L Average: {df['rougeLsum'].mean():.3f}\")\nprint(f\"BERTScore-F1 Average: {df['bert_f1'].mean():.3f}\")\n\n# 7. Save results\ndf.to_csv(\"evaluated_soap_notes.csv\", index=False)\nprint(\"\\nSaved results to 'evaluated_soap_notes.csv'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T19:59:30.734331Z","iopub.execute_input":"2025-05-24T19:59:30.735062Z","iopub.status.idle":"2025-05-24T19:59:53.263386Z","shell.execute_reply.started":"2025-05-24T19:59:30.735037Z","shell.execute_reply":"2025-05-24T19:59:53.262510Z"}},"outputs":[{"name":"stdout","text":"Creating temporary reference notes for demonstration...\n\nCalculating ROUGE scores...\nCalculating BERTScore (this may take a few minutes)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"319855c523da49e4a715422bd395de9b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b05866105f144f7b491e2615d26fba8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1298d7d1988b43de85a156a8fc11047a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3903b00672994745b6e82d809f70f959"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20a4b0a7bd6f4a6e96b08613a6a2e40d"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"979b37d002984aa09411716e6e6375cd"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"\nEvaluation Results (First 100 Reports):\nROUGE-1 Average: 0.026\nROUGE-2 Average: 0.009\nROUGE-L Average: 0.026\nBERTScore-F1 Average: 0.816\n\nSaved results to 'evaluated_soap_notes.csv'\n","output_type":"stream"}],"execution_count":11}]}